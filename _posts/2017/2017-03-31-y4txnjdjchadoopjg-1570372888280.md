---
layout: post
title:  "用4台虚拟机搭建成hadoop结构"
categories: 大数据
tags: hadoop 搭建
author: jiangc
excerpt: 用4台虚拟机搭建成hadoop结构
---
* content
{:toc}

用于测试，我用4台虚拟机搭建成了hadoop结构

![image](/images/2017\03\dsj\1570372888281.jpg "image")

我用了两个台式机。一个xp系统，一个win7系统。每台电脑装两个虚拟机，要不然内存就满了。

**1、安装虚拟机环境**

     Vmware，收费产品，占内存较大。

     或

     Oracle的VirtualBox，开源产品，占内存较小，但安装ubuntu过程中，重启会出错。

     我选Vmware。

**2、安装操作系统**

     Centos，红帽开源版，接近于生产环境。

     Ubuntu，操作简单，方便，界面友好。

     我选Ubuntu12.10.X 32位

**3、安装一些常用的软件**

     在每台linux虚拟机上，安装：vim，ssh

     sudo apt-get install vim

     sudo apt-get install ssh

     在客户端，也就是win7上，安装SecureCRT，Winscp或putty，这几个程序，都是依靠ssh服务来操作的，所以前提必须安装ssh服务。

     service ssh status 查看ssh状态。如果关闭使用service ssh start开启服务。

     SecureCRT，可以通过ssh远程访问linux虚拟机。

     winSCP或putty，可以从win7向linux上传文件。

**4、修改主机名和网络配置**

    主机名分别为：master，host2，host3，host4。

    sudo vim /etc/hostname

    网络配置，包括ip地址，子网掩码，DNS服务器。如上图所示。

**5、修改/etc/hosts文件。**

    修改每台电脑的hosts文件。

    hosts文件和windows上的功能是一样的。存储主机名和ip地址的映射。

    在每台linux上，sudo vim /etc/hosts 编写hosts文件。将主机名和ip地址的映射填写进去。编辑完后，结果如下：

![image](/images/2017\03\dsj\1570372888297.jpg "image")



**6、配置ssh，实现无密码登陆**

    无密码登陆，效果也就是在master上，通过 ssh host2 或 ssh host3 或 ssh host4 就可以登陆到对方计算机上。而且不用输入密码。

    四台虚拟机上，使用   ssh-keygen -t rsa    一路按回车就行了。

    刚才都作甚了呢？主要是设置ssh的密钥和密钥的存放路径。 路径为~/.ssh下。

    打开~/.ssh 下面有三个文件

    authorized\_keys，已认证的keys

    id\_rsa，私钥

    id\_rsa.pub，公钥   三个文件。

    下面就是关键的地方了，（我们要做ssh认证。进行下面操作前，可以先搜关于认证和加密区别以及各自的过程。）

    ①在master上将公钥放到authorized\_keys里。命令：sudo cat id\_rsa.pub \&gt;\&gt; authorized\_keys

    ②将master上的authorized\_keys放到其他linux的~/.ssh目录下。

       命令：sudo scp authorized\_keys [email protected]:~/.ssh

               sudo scp authorized\_keys 远程主机用户名@远程主机名或ip:存放路径。

    ③修改authorized\_keys权限，命令：chmod 644 authorized\_keys

    ④测试是否成功

       ssh host2 输入用户名密码，然后退出，再次ssh host2不用密码，直接进入系统。这就表示成功了。

**7、上传jdk，并配置环境变量。**

    通过winSCP将文件上传到linux中。将文件放到/usr/lib/java中，四个linux都要操作。

    解压缩：tar -zxvf jdk1.7.0\_21.tar

    设置环境变量 sudo vim ~/.bashrc

    在最下面添加：

    export JAVA\_HOME = /usr/lib/java/jdk1.7.0\_21

    export PATH = $JAVA\_HOME/bin:$PATH

    修改完后，用source ~/.bashrc让配置文件生效。

**8、上传hadoop，配置hadoop**

    通过winSCP，上传hadoop，到/usr/local/下，解压缩tar -zxvf hadoop1.2.1.tar

    再重命名一下，sudo mv hadoop1.2.1 hadoop

    这样目录就变成/usr/local/hadoop

     **①** 修改环境变量，将hadoop加进去（最后四个linux都操作一次）

    sudo vim ~/.bashrc

    export HADOOP\_HOME = /usr/local/hadoop

    export PATH = $JAVA\_HOme/bin:$HADOOP\_HOME/bin:$PATH

    修改完后，用source ~/.bashrc让配置文件生效。

     **②** 修改/usr/local/hadoop/conf下配置文件

    hadoop-env.sh，

![image](/images/2017\03\dsj\1570372888334.jpg "image")



    （上面这张图片，有一些问题，只export JAVA\_HOME进去就可以了，不用export HADOOP\_HOME和PATH了 ）

    core-site.xml，

![image](/images/2017\03\dsj\1570372888341.jpg "image")

    hdfs-site.xml，

![image](/images/2017\03\dsj\1570372888346.jpg "image")

    mapred-site.xml，

![image](/images/2017\03\dsj\1570372888352.jpg "image")

    master，

![image](/images/2017\03\dsj\1570372888360.jpg "image")

    slave，

![image](/images/2017\03\dsj\1570372888364.jpg "image")



    上面的hadoop-env.sh，core-site.xml，mapred-site.xml，hdfs-site.xml，master，slave几个文件，在四台linux中都是一样的。

    配置完一台电脑后，可以将hadoop包，直接拷贝到其他电脑上。

     **③** 最后要记得，将hadoop的用户加进去，命令为

      sudo chown -R hadoop:hadoop hadoop

      sudo chown -R 用户名@用户组 目录名

     **④** 让hadoop配置生效

      source hadoop-env.sh

     **⑤** 格式化namenode，只格式一次

      hadoop namenode -format

     **⑥** 启动hadoop

      切到/usr/local/hadoop/bin目录下，执行 start-all.sh启动所有程序

     **⑦** 查看进程，是否启动

       jps

      master，

![image](/images/2017\03\dsj\1570372888369.jpg "image")

      host2，

![image](/images/2017\03\dsj\1570372888376.jpg "image")

      host3，host4，的显示结果，与host2相同。
